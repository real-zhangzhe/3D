# SFT 监督微调
- 指令微调（Instruction Tuning）：利用构建的“指令-3D”配对数据，对预训练模型进行SFT。目标是让模型学会理解复杂的指令（如“生成一个穿蓝色上衣灰色裤子头发是红色的卡通企鹅”），减少幻觉。   

- 参数高效微调（PEFT/LoRA）：对于10B模型，全量微调成本高昂。采用LoRA（Low-Rank Adaptation）技术，仅微调注意力层的低秩矩阵，可减少99%的可训练参数，同时保持模型性能。这允许针对不同垂直领域（如“家具”、“二次元手办”）训练多个LoRA适配器，实现单模型多业务支持。   

- 灾难性遗忘的缓解：在微调3D任务时，模型可能会丧失通用的语言理解能力。解决方案是在训练数据中混合通用指令数据集（如Alpaca、Tulu），或采用重放（Replay）策略。
# RLHF / RLAIF 与 DPO：从人类 / AI 反馈中学习
单纯的SFT只能让模型“听懂”指令，而无法保证生成的“美感”或“物理合理性”。
- DPO：引入直接偏好优化（Direct Preference Optimization, DPO）。无需训练奖励模型，直接利用人类对生成结果的排序数据（Prefer A over B）来优化生成模型。这在解决3D生成的几何畸变、纹理模糊等非结构化问题上效果显著 。   

- RLHF / RLAIF：构建人类偏好奖励模型（Reward3D），对生成的3D资产进行打分，反馈给扩散模型进行强化学习微调，显著提升文本-3D的一致性和模型质量 。 
