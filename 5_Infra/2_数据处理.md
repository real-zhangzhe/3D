# 数据清洗与修复管线
数据预处理的质量直接决定了模型的上限。对于千万级数据，人工介入是不可能的，必须依赖算法自动化。
- 几何标准化：首先需统一文件格式（glb/obj -> usd/obj），并进行归一化（Normalization），将所有模型缩放至单位球体内，并校正重力轴（Up-axis alignment），确保训练时空间分布的一致性。   

- 拓扑修复与流形检查：大量从互联网爬取的3D模型（如Thingiverse）是为了3D打印设计的，但也存在面片缺失。需使用算法（如Watertight或Manifold库）检测并修复非流形几何结构。对于无法修复的模型（如G-code生成失败的模型），应直接丢弃以防污染训练集。   
- 渲染与多视图生成：由于绝大多数3D生成模型（如LRM）依赖2D监督信号，需要构建大规模分布式渲染集群（使用Blender Python API或Unreal Engine），为每个3D资产渲染12-32个随机视角的RGB图、深度图和法向图。这意味着2000万3D模型将衍生出数亿张2D图像，数据量级将达到PB级。   
# 自动化标注
原始3D数据往往缺乏高质量的文本描述。
- VLM反向标注：利用GPT-4V或开源多模态大模型（如LLaVA），对渲染出的多视角图像进行详细描述（Dense Captioning），生成包含几何特征、材质、风格的文本标签。这不仅解决了文本对齐问题，还通过引入丰富的语义信息，增强了模型对复杂指令的理解能力。   
# 存储与I/O优化
在训练过程中，数千万小文件（Mesh/Texture）的读取是I/O噩梦。
- WebDataset格式：将数千个样本打包为Tar包，支持流式读取（Streaming），避免随机小文件读取导致的磁盘IOPS瓶颈。

- 数据分层：将高频访问的Latent特征存储在NVMe SSD阵列，冷数据存储在对象存储（S3/OSS），并通过Alluxio等缓存层加速读取。
