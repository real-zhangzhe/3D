# 3D 生成模型发展
## 纯 3D 重建
纯 3D 重建的意思是只能将物理世界已经存在的实物转成 3D 资产
- [NeRF](https://github.com/real-zhangzhe/3D/blob/main/1_3D%E7%94%9F%E6%88%90/1_3D%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.md#2-%E7%A5%9E%E7%BB%8F%E8%BE%90%E5%B0%84%E5%9C%BA-neural-radiance-fields-nerf)
- [3DGS](https://github.com/real-zhangzhe/3D/blob/main/1_3D%E7%94%9F%E6%88%90/1_3D%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.md#4-3d-%E9%AB%98%E6%96%AF%E6%BA%85%E5%B0%84-3d-gaussian-splatting-3dgs)
## 基于大模型的 3D 生成
以 Hunyuan3D 为例，Hunyuan3D 共有三个大版本，1.0 / 2.0 / omni
- [hunyuan3D 1.0](https://arxiv.org/pdf/2411.02293)
- [hunyuan3D 2.0](https://arxiv.org/pdf/2501.12202)
- [hunyuan3D Omni](https://arxiv.org/pdf/2509.21245)
- 进化脉络总览
  - **Hunyuan3D 1.0**: 采用“多视角生成 + 稀疏视图重建”的两阶段方案，本质上是一个 2D 提升 (2D-Lifting) 框架，通过强大的2D扩散模型生成中间图像，再重建为3D模型。
  - **Hunyuan3D 2.0**: 架构发生根本性变革，转向 原生3D生成 (3D-Native) 框架。它直接在3D数据的潜在空间中进行扩散生成，分为“形状生成”和“纹理生成”两个独立的阶段，标志着技术路线的重大升级。
  - **Hunyuan3D-Omni**: 在2.0的强大原生3D生成能力基础上，增加了精细化、多模态的控制能力。它通过一个统一的控制器，接受点云、骨骼、体素等多种几何先验信息，实现了对生成结果的精准操控。
| 特性 | Hunyuan3D 1.0 | Hunyuan3D 2.0| Hunyuan3D-Omni |
| :--- | :--- | :--- | :--- |
| 核心范式 | 2D提升 (2D-Lifting) | 原生3D生成 (3D-Native) | 可控原生3D生成 |
| 主要输入 | 单张图片 / 文本 | 单张图片 | 图片 + 控制信号 (点云/体素/边界框/骨骼) |
| 主要输出 | 3D网格 | 带 PBR材质 的纹理化3D网格 | 符合控制条件的3D网格 |
| 阶段一 | 多视角图像生成 | 形状生成 (Shape Generation) | 形状生成 (集成控制信号) |
| 阶段二 | 稀疏视图3D重建 | 纹理合成 (Texture Synthesis) | (继承2.0的纹理合成能力) |
| 关键技术 | 混合输入、0度仰角轨道 | ShapeVAE、重要性采样、DiT、Paint模型、去光照 | 统一控制编码器 (Unified Control Encoder) |
| 进化方向 | 提高生成速度和泛化性 | 提升模型原生3D能力和输出质量 | 增加生成过程的可控性和精确度 |
### Hunyuan3D 1.0（2D-Lifting 框架）
![hunyuan1.0.png](https://s2.loli.net/2025/09/29/HWxhQEZIdAr3ayT.png)
- 开源，参数量 7.18B
- 核心功能：一个统一的框架，支持从文本或单张图片生成3D资产，旨在实现速度与质量的平衡。
- 输入与输出
  - 输入: 单一文本提示或单张图片。
  - 输出: 带有UV贴图的3D网格模型，可以直接被艺术家使用。
- 模型阶段与流程：一个明确的两阶段流水线。
  - 第一阶段：多视角扩散模型 (Multi-view Diffusion)
    - 输入: 条件图片或文本提示。
    - 功能: 利用一个微调后的大型2D扩散模型（如Hunyuan-DiT），高效地生成一组（例如6张）围绕物体的、视角一致的RGB图像。
    - 输出: 一组多视角RGB图像。
  - 第二阶段：稀疏视图重建模型 (Sparse-view Reconstruction)
    - 输入: 第一阶段生成的多视角图像，以及作为辅助信息的 原始条件图片（这被称为“混合输入”，是其创新点之一）。
    - 功能: 一个前馈式的Transformer模型，接收带有已知相机位姿的多视角图像和无位姿信息的条件图像，快速重建出3D形状。
    - 输出: 3D模型。模型内部使用符号距离函数（SDF）表示，并通过移动立方体算法（Marching Cubes）转换为显式的网格模型。
- 训练数据构造
  - 来源: 使用了类似于 [Objaverse-xl（开源，1000 万个 3D 物体，每条数据主要包含两个模态的信息：文本（这个物体是什么），3D（形状和纹理））](https://huggingface.co/datasets/allenai/objaverse-xl) 的大规模内部数据集。数据经过筛选，移除了复杂场景或劣质模型。
  - 条件图渲染: 相机位姿随机采样（例如，仰角在[-20°, 60°]之间），并使用随机的HDR环境光。
  - 目标多视角图渲染: 为了最大化生成视图间的可见区域，相机被固定在 0度仰角 的轨道上，并沿方位角均匀采样（如0°, 60°, 120°...）。
### Hunyuan3D 2.0  (原生3D生成框架)
![hunyuan2.0.png](https://s2.loli.net/2025/09/29/ToR7JhSW9jvyLrV.png)
- 开源，参数量没有查到。
- 核心功能：一个先进的大规模3D合成系统，可从单张图片生成高分辨率、带PBR（基于物理的渲染）材质的纹理化3D资产。
- 输入与输出
  - 输入: 单张图片。
  - 输出: 高保真度的3D网格，以及一套PBR材质贴图（如反照率Albedo、金属度Metallic、粗糙度Roughness）。
- 模型阶段与流程：同样是两阶段，但阶段的定义与1.0完全不同，解耦了形状和纹理的生成。
  - 第一阶段：形状生成 (Shape Generation - Hunyuan3D-DiT) 此阶段本身包含两个核心组件：
      - Hunyuan3D-ShapeVAE (自编码器): 这是一个预训练模型，用于将3D网格数据压缩成紧凑的潜在空间表征（一系列连续的tokens）。其关键创新是除了均匀采样，还引入了表面重要性采样，在模型的边和角等细节丰富区域采集更多点，以更好地捕捉几何细节。
      - Hunyuan3D-DiT (扩散模型):
        - 输入: 单张图片。
        - 功能: 一个基于流匹配（Flow-based）的扩散Transformer模型，它在ShapeVAE构建的潜在空间中进行操作，根据输入图片预测出代表3D形状的潜在tokens序列。
        - 输出: 潜在tokens序列。该序列随后被送入ShapeVAE的解码器，解码成最终的3D网格。
  - 第二阶段：纹理合成 (Texture Synthesis - Hunyuan3D-Paint)
    - 输入: 第一阶段生成的无纹理网格，以及原始的条件图片。
    - 功能: 一个以网格为条件的多视角图像生成模型。它首先从输入网格渲染出法线图、坐标图等几何先验信息，然后结合条件图片，生成一组在多个视角下都与几何体对齐且相互一致的图像。该模型还包含一个图像去光照 (Image Delighting) 模块，用于消除输入图片中的光影，以生成与光照无关的纹理。
    - 输出: 一张高分辨率、无缝的PBR纹理贴图。这是通过将生成的多视角图像“烘焙”(Baking)到网格的UV上实现的。
- 训练数据构造
  - 形状数据: 约10万个来自公共数据集（如ShapeNet, Objaverse）的3D模型。预处理流程非常精细，包括归一化、水密化处理 (Watertight)、SDF采样、混合表面采样等步骤。
  - 纹理数据: 约7万个从Objaverse-XL中筛选出的高质量、经人工标注的3D模型。为每个模型从多个角度渲染PBR贴图（反照率、金属度、粗糙度）作为训练目标。
### Hunyuan3D-Omni (可控原生3D生成框架)
- 核心功能：在Hunyuan3D 2.0的基础上构建的统一框架，用于实现精细化、可控的3D资产生成。
- 输入与输出
  - 输入: 多模态输入，包括一张基础图片 加上 一种额外的控制信号。支持的控制信号有：
    - 点云 (Point Cloud) 
    - 体素 (Voxel) 
    - 边界框 (Bounding Box) 
    - 骨骼姿态 (Skeleton) 
- 输出: 一个在几何、拓扑或姿态上同时遵循图片内容和额外控制信号的3D网格。
- 模型阶段与流程
  - 模型的主体架构继承自Hunyuan3D 2.0（DiT + VAE解码器），其核心创新在于如何将控制信号融入生成过程。
  - 新增模块：统一控制编码器 (Unified Control Encoder)
  - 输入: 上述四种控制信号中的一种。
  - 功能: 这是一个轻量级的共享编码器。它的设计巧妙之处在于，将所有控制信号都统一表示为点云形式（例如，边界框的8个顶点、骨骼的关节点、体素的中心点）。编码器处理这些“点云”并提取其特征。同时，模型还使用一个任务嵌入（Task Embedding）来区分当前输入的是哪种类型的控制，避免混淆。
  - 输出: 控制信号的特征嵌入 (feature embedding)。
- 特征融合与生成：从“统一控制编码器”输出的特征嵌入，会与从输入图片中提取的DINO特征进行拼接 (concatenate)。这个融合后的特征向量作为最终的条件，被送入Hunyuan3D-DiT模型中，引导扩散过程生成符合所有条件的3D形状潜在tokens。后续流程与2.0一致，通过VAE解码器得到最终网格。
- 训练数据构造
  - 来源: 基于Hunyuan3D 2.0的数据集。
  - 控制信号构造: 在训练时，从原始的3D模型中提取或生成相应的控制信号。例如，从模型表面采样点云，计算其边界框，或从带骨骼的动画角色中提取某一帧的骨骼和对应的网格。
  - 训练策略: 采用了一种“难度感知采样策略”，在训练中会更频繁地采样像姿态控制这样比较难学习的任务，以保证模型能稳健地掌握所有控制能力。

